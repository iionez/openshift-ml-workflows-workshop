{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will briefly present *Bayesian spam filtering*, which is an established and effective technique for spam filtering.  The basic idea is to look at documents as *bags of words* (that is, as mappings of words to frequencies, disregarding ordering).  The underlying assumption is that spam and legitimate documents will have different distributions of words, and that we'll be able to rate the probability that a given set of words came from a legitimate document or a spam document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os.path\n",
    "\n",
    "data = pd.read_parquet(os.path.join(\"data\", \"training.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by splitting our data into randomly-selected train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "train, test = model_selection.train_test_split(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our feature extraction pipeline is very simple:  we'll use a _bag-of-words_ model in which we represent texts as dictionaries of word counts.  Furthermore, we'll use feature hashing so that we store word counts in a array of hash buckets rather than as an explicit dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text as text_feature\n",
    "\n",
    "hv = text_feature.HashingVectorizer(norm='l1', alternate_sign=False)\n",
    "hashed_features = hv.transform(train.text.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From there, we'll use the multinomial naive Bayes classifier in scikit-learn, which will train a model to identify which words (really, which hash values of words) are most likely to distinguish between legitimate and spam messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import naive_bayes\n",
    "\n",
    "nb = naive_bayes.MultinomialNB()\n",
    "nb.fit(hashed_features, train.label.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've fit our model, we can evaluate its accuracy on our training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9779666666666667"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.score(hashed_features, train.label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9765"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features = hv.transform(test.text.values)\n",
    "nb.score(test_features, test.label.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you know, raw accuracy isn't the most useful metric to evaluate a binary classifier.  In order to visualize how the naive Bayes classifier performs overall, we'll plot a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlworkflows import plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, chart = plot.binary_confusion_matrix(test.label.values, nb.predict(test_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "const spec = {\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-0c5279cb657406fc45f807e031fcc85f\"}, \"mark\": \"rect\", \"encoding\": {\"color\": {\"type\": \"quantitative\", \"field\": \"value\"}, \"tooltip\": [{\"type\": \"quantitative\", \"field\": \"raw_count\"}], \"x\": {\"type\": \"ordinal\", \"field\": \"predicted\"}, \"y\": {\"type\": \"ordinal\", \"field\": \"actual\"}}, \"height\": 215, \"width\": 215, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-0c5279cb657406fc45f807e031fcc85f\": [{\"predicted\": \"legitimate\", \"actual\": \"legitimate\", \"raw_count\": 4737, \"value\": 0.9591010325976919}, {\"predicted\": \"legitimate\", \"actual\": \"spam\", \"raw_count\": 202, \"value\": 0.04089896740230816}, {\"predicted\": \"spam\", \"actual\": \"legitimate\", \"raw_count\": 33, \"value\": 0.006520450503852994}, {\"predicted\": \"spam\", \"actual\": \"spam\", \"raw_count\": 5028, \"value\": 0.993479549496147}]}};\n",
       "const opt = {};\n",
       "const type = \"vega-lite\";\n",
       "const id = \"be475f19-b28d-4955-bd8f-dcb54730fd15\";\n",
       "\n",
       "const output_area = this;\n",
       "\n",
       "require([\"nbextensions/jupyter-vega/index\"], function(vega) {\n",
       "  const target = document.createElement(\"div\");\n",
       "  target.id = id;\n",
       "  target.className = \"vega-embed\";\n",
       "\n",
       "  const style = document.createElement(\"style\");\n",
       "  style.textContent = [\n",
       "    \".vega-embed .error p {\",\n",
       "    \"  color: firebrick;\",\n",
       "    \"  font-size: 14px;\",\n",
       "    \"}\",\n",
       "  ].join(\"\\\\n\");\n",
       "\n",
       "  // element is a jQuery wrapped DOM element inside the output area\n",
       "  // see http://ipython.readthedocs.io/en/stable/api/generated/\\\n",
       "  // IPython.display.html#IPython.display.Javascript.__init__\n",
       "  element[0].appendChild(target);\n",
       "  element[0].appendChild(style);\n",
       "\n",
       "  vega.render(\"#\" + id, spec, type, opt, output_area);\n",
       "}, function (err) {\n",
       "  if (err.requireType !== \"scripterror\") {\n",
       "    throw(err);\n",
       "  }\n",
       "});\n"
      ],
      "text/plain": [
       "<vega.vegalite.VegaLite at 0x11d1cfbe0>"
      ]
     },
     "metadata": {
      "jupyter-vega": "#be475f19-b28d-4955-bd8f-dcb54730fd15"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also inspect the performance in tabular form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted</th>\n",
       "      <th>actual</th>\n",
       "      <th>raw_count</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>legitimate</td>\n",
       "      <td>legitimate</td>\n",
       "      <td>4737</td>\n",
       "      <td>0.959101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>legitimate</td>\n",
       "      <td>spam</td>\n",
       "      <td>202</td>\n",
       "      <td>0.040899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>legitimate</td>\n",
       "      <td>33</td>\n",
       "      <td>0.006520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spam</td>\n",
       "      <td>spam</td>\n",
       "      <td>5028</td>\n",
       "      <td>0.993480</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    predicted      actual  raw_count     value\n",
       "0  legitimate  legitimate       4737  0.959101\n",
       "1  legitimate        spam        202  0.040899\n",
       "2        spam  legitimate         33  0.006520\n",
       "3        spam        spam       5028  0.993480"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can produce a report showing us the precision and recall for each class, as well as an [f<sub>1</sub>-score](https://en.wikipedia.org/wiki/F1_score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  legitimate       0.99      0.96      0.98      4939\n",
      "        spam       0.96      0.99      0.98      5061\n",
      "\n",
      "    accuracy                           0.98     10000\n",
      "   macro avg       0.98      0.98      0.98     10000\n",
      "weighted avg       0.98      0.98      0.98     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test.label.values, nb.predict(test_features)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "0.  The preprocessing step in this notebook is virtually nonexistent.  How might you use spaCy to have a more robust preprocessing pipeline?\n",
    "1.  The feature extraction pipeline in this notebook uses hashed vectors with 2<sup>20</sup> elements.  Run some experiments to identify whether or not smaller vectors would still provide acceptable performance.\n",
    "2.  Bayesian document classification is an established technique that has worked well in practice for a long time.  A common way to fool Bayesian spam filters is to append a lot of legitimate text to the end of a spam document.  What sort of features would you extract in order to train a classifier that could identify spam messages that were using this trick?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
