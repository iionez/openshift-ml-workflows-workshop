{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Perhaps you've played [Twenty Questions](https://en.wikipedia.org/wiki/Twenty_Questions) before:  it's a game where one player (the *answerer*) thinks of a person, place, or thing, and other players ask yes-or-no questions to guess the object of the answerer's thoughts.  Since the answerer probably knows about a lot of different people and objects, a good strategy for the other players involves devising questions that reduce the space of possible answers as much as possible no matter how they are answered.  \n",
    "\n",
    "Given a labeled collection of examples, you might imagine a technique to [learn a *decision tree*](https://en.wikipedia.org/wiki/Decision_tree_learning) of questions to classify these examples by asking as few questions as possible.  However, you might imagine that such a technique would necessarily be quite dependent on the exact examples on offer.  (In other words,  these techniques are prone to *overfitting*.)  As a simple illustration,  consider the case where your set of example objects was `{ 'ant', 'elephant'}`.  In this case, the question \"is it smaller than a typical adult human\" would enable you to differentiate between examples optimally.   However, that question would be useless if our set of example objects was the set of all domesticated dog breeds.\n",
    "\n",
    "[Random decision forest models](https://en.wikipedia.org/wiki/Random_forest) work by training an *ensemble* of imprecise decision trees that only consider subsets of features or examples and then aggregating the results from the ensemble.  By learning and aggregating an ensemble of trees, random decision forests can be more accurate than individual decision trees *and* are less likely to overfit.  In this notebook, we'll use a random decision forest to classify documents as either \"spam\" (based on food reviews) or \"legitimate\" (based on Jane Austen).\n",
    "\n",
    "We'll begin by loading in the feature vectors which we generated in either [the simple summaries feature extraction notebook](03-feature-engineering-summaries.ipynb) or [the TF-IDF feature extraction notebook](03-feature-engineering-tfidf.ipynb). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os.path\n",
    "\n",
    "features = pd.read_parquet(os.path.join(\"data\", \"features.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When doing exploratory analysis, it's often a good idea to inspect your data as a sanity check.  In this case, we'll make sure that the feature vectors we generated in the last notebook have the shape we expect!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>label</th>\n",
       "      <th>no_punct</th>\n",
       "      <th>number_words</th>\n",
       "      <th>mean_wl</th>\n",
       "      <th>max_wl</th>\n",
       "      <th>min_wl</th>\n",
       "      <th>pc_10_wl</th>\n",
       "      <th>pc_90_wl</th>\n",
       "      <th>upper</th>\n",
       "      <th>stop_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24198</th>\n",
       "      <td>4198</td>\n",
       "      <td>spam</td>\n",
       "      <td>15</td>\n",
       "      <td>109</td>\n",
       "      <td>3.981651</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17083</th>\n",
       "      <td>17083</td>\n",
       "      <td>legitimate</td>\n",
       "      <td>12</td>\n",
       "      <td>105</td>\n",
       "      <td>4.209524</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>15</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24222</th>\n",
       "      <td>4222</td>\n",
       "      <td>spam</td>\n",
       "      <td>12</td>\n",
       "      <td>67</td>\n",
       "      <td>4.164179</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25874</th>\n",
       "      <td>5874</td>\n",
       "      <td>spam</td>\n",
       "      <td>8</td>\n",
       "      <td>73</td>\n",
       "      <td>4.123288</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>13</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>529</td>\n",
       "      <td>legitimate</td>\n",
       "      <td>12</td>\n",
       "      <td>52</td>\n",
       "      <td>4.576923</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       index       label  no_punct  number_words   mean_wl  max_wl  min_wl  \\\n",
       "24198   4198        spam        15           109  3.981651      10       1   \n",
       "17083  17083  legitimate        12           105  4.209524      10       1   \n",
       "24222   4222        spam        12            67  4.164179      11       1   \n",
       "25874   5874        spam         8            73  4.123288      10       1   \n",
       "529      529  legitimate        12            52  4.576923      10       2   \n",
       "\n",
       "       pc_10_wl  pc_90_wl  upper  stop_words  \n",
       "24198       2.0       7.0     11          65  \n",
       "17083       2.0       7.0     15          53  \n",
       "24222       2.0       7.0     10          30  \n",
       "25874       2.0       7.0     13          40  \n",
       "529         2.0       8.0      7          28  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import model_selection\n",
    "\n",
    "train, test = model_selection.train_test_split(features)\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=25, random_state=404)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=25, random_state=404)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_FEATURES=512\n",
    "rfc.fit(X=train.iloc[:,2:train.shape[1]], y=train[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlworkflows import plot\n",
    "\n",
    "predictions = rfc.predict(test.iloc[:,2:train.shape[1]])\n",
    "df, chart = plot.binary_confusion_matrix(test[\"label\"], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-275f1e9cfe634f2eafdcf0d8d0d3c3d6\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-275f1e9cfe634f2eafdcf0d8d0d3c3d6\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-275f1e9cfe634f2eafdcf0d8d0d3c3d6\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-ca649fd79196f52b006c813d8e835c99\"}, \"mark\": \"rect\", \"encoding\": {\"color\": {\"type\": \"quantitative\", \"field\": \"value\"}, \"tooltip\": [{\"type\": \"quantitative\", \"field\": \"raw_count\"}], \"x\": {\"type\": \"ordinal\", \"field\": \"predicted\"}, \"y\": {\"type\": \"ordinal\", \"field\": \"actual\"}}, \"height\": 215, \"width\": 215, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-ca649fd79196f52b006c813d8e835c99\": [{\"actual\": \"legitimate\", \"predicted\": \"legitimate\", \"raw_count\": 3990, \"value\": 0.7862068965517242}, {\"actual\": \"legitimate\", \"predicted\": \"spam\", \"raw_count\": 1085, \"value\": 0.21379310344827587}, {\"actual\": \"spam\", \"predicted\": \"legitimate\", \"raw_count\": 863, \"value\": 0.17522842639593908}, {\"actual\": \"spam\", \"predicted\": \"spam\", \"raw_count\": 4062, \"value\": 0.824771573604061}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the raw numbers, and proportions of correctly and incorrectly classified items: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>predicted</th>\n",
       "      <th>raw_count</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>legitimate</td>\n",
       "      <td>legitimate</td>\n",
       "      <td>3990</td>\n",
       "      <td>0.786207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>legitimate</td>\n",
       "      <td>spam</td>\n",
       "      <td>1085</td>\n",
       "      <td>0.213793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>legitimate</td>\n",
       "      <td>863</td>\n",
       "      <td>0.175228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spam</td>\n",
       "      <td>spam</td>\n",
       "      <td>4062</td>\n",
       "      <td>0.824772</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       actual   predicted  raw_count     value\n",
       "0  legitimate  legitimate       3990  0.786207\n",
       "1  legitimate        spam       1085  0.213793\n",
       "2        spam  legitimate        863  0.175228\n",
       "3        spam        spam       4062  0.824772"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the precision, recall and f1-score for the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  legitimate       0.82      0.79      0.80      5075\n",
      "        spam       0.79      0.82      0.81      4925\n",
      "\n",
      "    accuracy                           0.81     10000\n",
      "   macro avg       0.81      0.81      0.81     10000\n",
      "weighted avg       0.81      0.81      0.81     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test.label.values, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One interesting aspect of random decision forests is that they provide a metric for how important each feature was to the ultimate conclusion.  This is a useful property both for having *explainable models* (i.e., so you can explain to a human why the model made a particular prediction) and for guiding further experiments (i.e., so you can learn more about the real world based on what the model has identified as likely to be correlated with what you're trying to predict)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = list(enumerate(rfc.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8, 0.22659533731383058),\n",
       " (1, 0.19883678397461438),\n",
       " (2, 0.16625898179699608),\n",
       " (6, 0.1242284501583237),\n",
       " (0, 0.09178735183510703),\n",
       " (7, 0.08968010722337802),\n",
       " (3, 0.06664270469829912),\n",
       " (5, 0.02633237595586103),\n",
       " (4, 0.009637907043590103)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.sort(key=lambda x: -x[1])\n",
    "l[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What these features actually *mean* depends on which feature engineering approach you chose in the last notebook.  In the case of the simple summaries approach, it's fairly straightforward -- we can just take the column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop_words 0.22659533731383058\n",
      "number_words 0.19883678397461438\n",
      "mean_wl 0.16625898179699608\n",
      "pc_90_wl 0.1242284501583237\n",
      "no_punct 0.09178735183510703\n",
      "upper 0.08968010722337802\n",
      "max_wl 0.06664270469829912\n",
      "pc_10_wl 0.02633237595586103\n",
      "min_wl 0.009637907043590103\n"
     ]
    }
   ],
   "source": [
    "if len(l) < 20:\n",
    "    # the simple summaries have fewer than 20 features\n",
    "    d = dict(enumerate(train.columns[2:]))\n",
    "    for k, v in l[:20]:\n",
    "        print(d[k], v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you used the simple summaries approach, you'll probably see that `stop_words` is the most important feature.\n",
    "\n",
    "If you used the tf-idf approach, the preceding cell shouldn't have produced any output and we'll need a more involved approach.  Recall that we used [feature hashing](https://en.wikipedia.org/wiki/Feature_hashing) for the tf-idf feature vectors.  Feature hashing has many benefits, but one of the downsides is that hashing is one-way -- we can't go from the entry in the feature vector to the word that produced it.  However, we can do a little extra work to try and reconstruct the words that _might_ have produced such a vector.\n",
    "\n",
    "The size of our feature vectors is smaller than the number of words we might have seen in our corpus of documents, [which means that more than one word will be represented by counts in a given bucket](https://en.wikipedia.org/wiki/Pigeonhole_principle).  But we can construct a list of the words that may have mapped to each bucket by getting a vocabulary for our document corpus and then hashing each word, which we can use to identify which words _may_ have corresponded to the most important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(l) > 20:\n",
    "    from sklearn.utils.murmurhash import murmurhash3_bytes_s32\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from collections import defaultdict\n",
    "    \n",
    "    def fhash(v, size = 1024):\n",
    "        return murmurhash3_bytes_s32(v.encode(\"utf-8\"), 0) % size\n",
    "    data = pd.read_parquet(\"data/training.parquet\")\n",
    "    vectorizer = CountVectorizer(token_pattern='(?u)\\\\b[A-Za-z]\\\\w+\\\\b')\n",
    "    vectorizer.fit(data[\"text\"])\n",
    "    d = defaultdict(lambda: [])\n",
    "    for k in vectorizer.vocabulary_.keys():\n",
    "        d[fhash(k)].append(k)\n",
    "    \n",
    "    for k, v in l[:20]:\n",
    "        print(d[k], v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sort of investigation is useful to see how well our feature count is working in the context of feature hashing:  if we see that an important feature has many apparently-unrelated words (especially if many relatively-common words are in the same bucket), we may want to increase the number of buckets to improve the performance of our model.\n",
    "\n",
    "Finally, we'll want to save the model so that we can use it outside of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlworkflows import util\n",
    "util.serialize_to(rfc, \"model.sav\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
